{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "LANGUAGE_KEY = os.getenv('LANGUAGE_KEY')\n",
    "LANGUAGE_ENDPOINT = os.getenv('LANGUAGE_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx \n",
    "\n",
    "# doc =  docx.Document(\"data/2024_12_06_Réunion d'équipe.docx\")\n",
    "doc =  docx.Document(\"data/2024-12-208Réunion-d'équipe.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    print(doc.paragraphs[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predefined Roles:\n",
    "\n",
    "\"Customer\":\n",
    "\"Agent\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = (\"list of names from the transcript file\")\n",
    "company = dict({\"name1\":{'id':1,\n",
    "                                    'role':'Customer'},\n",
    "                  \"name2\":{'id':2,\n",
    "                                   'role':'Customer'},\n",
    "                  \"name3\":{'id':3,\n",
    "                                   'role':'Assistant'},\n",
    "                  \"name4\":{'id':4,\n",
    "                                    'role':'Customer'},                    \n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38419 characters\n",
      "9604.75 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def assign_role_and_id(line):\n",
    "    nom = \" \".join(line.split(\" \")[:2])\n",
    "    participant_id = company[nom][\"id\"]\n",
    "    role = company[nom][\"role\"]\n",
    "    return participant_id, role\n",
    "\n",
    "formatted_paragraphs=list()\n",
    "characters_count=0\n",
    "for i, paragraph in enumerate(doc.paragraphs):\n",
    "    text=list()\n",
    "    if i <= 2:\n",
    "        continue\n",
    "    # Format each paragraph as a dictionary\n",
    "    for line in paragraph.text.split(\"\\n\"):\n",
    "        \n",
    "        if line == \"\":\n",
    "            continue\n",
    "        \n",
    "        elif line.startswith(people):  \n",
    "            participant_id, role = assign_role_and_id(line)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            if line:\n",
    "                text.append(line.replace(\"\\xa0\",\"\"))\n",
    "                \n",
    "                characters_count += len(line.replace(\"\\xa0\",\"\"))\n",
    "            \n",
    "    formatted_paragraph = {\n",
    "        \"text\": \". \".join(text),\n",
    "        \"id\": str(i),  # Assign a unique ID based on the order\n",
    "        \"role\": role,      # Assign a role (adjust based on your document content)\n",
    "        \"participantId\": participant_id\n",
    "    }\n",
    "    \n",
    "    # Append formatted paragraph to list\n",
    "    formatted_paragraphs.append(formatted_paragraph)\n",
    "    \n",
    "print(characters_count,\"characters\")\n",
    "print(characters_count/4,\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([formatted_paragraphs[i]['text'] for i in range(len(formatted_paragraphs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "sum(1 for _ in unicodedata.normalize('NFC', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def get_text_length(text):\n",
    "    \"\"\"Calculate the length of a string in Unicode text elements.\"\"\"\n",
    "    return sum(1 for _ in unicodedata.normalize('NFC', text))\n",
    "\n",
    "def chunk_text_by_length(conversation, max_length):\n",
    "    \"\"\"Split the conversation into chunks based on text length in Unicode elements.\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for item in conversation:\n",
    "        text_length = get_text_length(item['text'])\n",
    "        if current_length + text_length <= max_length:\n",
    "            current_chunk.append(item)\n",
    "            current_length += text_length\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [item]\n",
    "            current_length = text_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text_by_length(formatted_paragraphs,max_length=12500)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_result(result):\n",
    "    task_results = result[\"tasks\"][\"items\"]\n",
    "    for task in task_results:\n",
    "        print(f\"\\n{task['taskName']} status: {task['status']}\")\n",
    "        task_result = task[\"results\"]\n",
    "        if task_result[\"errors\"]:\n",
    "            print(\"... errors occurred ...\")\n",
    "            for error in task_result[\"errors\"]:\n",
    "                print(error)\n",
    "        else:\n",
    "            conversation_result = task_result[\"conversations\"][0]\n",
    "            if conversation_result[\"warnings\"]:\n",
    "                print(\"... view warnings ...\")\n",
    "                for warning in conversation_result[\"warnings\"]:\n",
    "                    print(warning)\n",
    "            else:\n",
    "                summaries = conversation_result[\"summaries\"]\n",
    "                for summary in summaries:\n",
    "                    print(f\"{summary['aspect']}: {summary['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "\n",
    "# conversation_chunks = chunk_text_by_length(formatted_paragraphs,max_length=4500)\n",
    "\n",
    "conversation_chunks = chunks[1:]\n",
    "\n",
    "client = ConversationAnalysisClient(LANGUAGE_ENDPOINT, AzureKeyCredential(LANGUAGE_KEY))\n",
    "\n",
    "try:\n",
    "    results = []\n",
    "    for idx, chunk in enumerate(conversation_chunks):\n",
    "        conversation_id = f\"meeting_group{idx + 1}\"\n",
    "        poller = client.begin_conversation_analysis(\n",
    "            task={\n",
    "                \"displayName\": f\"Analyze conversations part {idx + 1}\",\n",
    "                \"analysisInput\": {\n",
    "                    \"conversations\": [\n",
    "                        {\n",
    "                            \"conversationItems\": chunk,\n",
    "                            \"modality\": \"text\",\n",
    "                            \"id\": conversation_id,\n",
    "                            \"language\": \"fr\",\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "                \"tasks\": [\n",
    "                    # {\n",
    "                    #     \"taskName\": \"Issue task\",\n",
    "                    #     \"kind\": \"ConversationalSummarizationTask\",\n",
    "                    #     \"parameters\": {\"summaryAspects\": [\"issue\"]},\n",
    "                    # },\n",
    "                    # {\n",
    "                    #     \"taskName\": \"Resolution task\",\n",
    "                    #     \"kind\": \"ConversationalSummarizationTask\",\n",
    "                    #     \"parameters\": {\"summaryAspects\": [\"resolution\"]},\n",
    "                    # },\n",
    "         \n",
    "                    {\n",
    "                        \"taskName\": \"Title and chapter task\",\n",
    "                        \"kind\": \"ConversationalSummarizationTask\",\n",
    "                        \"parameters\": {\"summaryAspects\": [\"chapterTitle\"]},\n",
    "                    },\n",
    "                    {\n",
    "                        \"taskName\": \"Recap task narrative\",\n",
    "                        \"kind\": \"ConversationalSummarizationTask\",\n",
    "                        \"parameters\": {\"summaryAspects\": [\"narrative\"]},\n",
    "                    },\n",
    "                    {\n",
    "                        \"taskName\": \"Recap task\",\n",
    "                        \"kind\": \"ConversationalSummarizationTask\",\n",
    "                        \"parameters\": {\"summaryAspects\": [\"recap\"]},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        result = poller.result()\n",
    "        results.append(result)\n",
    "        view_result(result)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "finally:\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create doc document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "\n",
    "recap = Document()\n",
    "\n",
    "recap.add_heading('Resume equipe 2024-12-20')\n",
    "recap.add_paragraph(\"Ce resume utilise l'outil Microsoft Azure ConversationAnalysisClient avec les paramètres 'summaryAspects': [chapterTitle] & [narrative]\")\n",
    "for i, result in enumerate(results):\n",
    "\n",
    "    task_results = result[\"tasks\"][\"items\"]\n",
    "    for task in task_results:\n",
    "        print(f\"\\n{task['taskName']} status {i+1}: {task['status']}\")\n",
    "        recap.add_heading(f\"\\n{task['taskName']} status {i}: {task['status']}\", level=1)\n",
    "        task_result = task[\"results\"]\n",
    "        if task_result[\"errors\"]:\n",
    "            print(\"... errors occurred ...\")\n",
    "            for error in task_result[\"errors\"]:\n",
    "                print(error)\n",
    "        else:\n",
    "            conversation_result = task_result[\"conversations\"][0]\n",
    "            if conversation_result[\"warnings\"]:\n",
    "                print(\"... view warnings ...\")\n",
    "                for warning in conversation_result[\"warnings\"]:\n",
    "                    print(warning)\n",
    "            else:\n",
    "                summaries = conversation_result[\"summaries\"]\n",
    "                for summary in summaries:\n",
    "                    print(f\"{summary['aspect']} {i+1}: {summary['text']}\")\n",
    "                    recap.add_paragraph(f\"{summary['aspect']} {i}: {summary['text']}\")\n",
    "                    \n",
    "                    \n",
    "recap.save(\"2024-12-20_recap_reunion_equipe.docx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature\tValue\n",
    "\n",
    "Text Analytics for health\t125,000 characters as measured by StringInfo.LengthInTextElements.\n",
    "\n",
    "All other preconfigured features (synchronous)\t5,120 as measured by StringInfo.LengthInTextElements. If you need to submit larger documents, consider using the feature asynchronously.\n",
    "\n",
    "All other preconfigured features (asynchronous)\t125,000 characters across all submitted documents, as measured by StringInfo.LengthInTextElements (maximum of 25 documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text_by_length(formatted_paragraphs,max_length=125000)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.ai.language.conversations.aio import ConversationAnalysisClient\n",
    "# import asyncio\n",
    "\n",
    "# async def analyze_conversations_async(language_endpoint, language_key, conversation_chunk):\n",
    "#     client = ConversationAnalysisClient(language_endpoint, AzureKeyCredential(language_key))\n",
    "\n",
    "#     try:\n",
    "#         conversation_id = f\"meeting_summary_group_meeting_{ 1}\"\n",
    "        \n",
    "#         # Use `await` for asynchronous operations\n",
    "#         poller = await client.begin_conversation_analysis(\n",
    "#             task={\n",
    "#                 \"displayName\": f\"Analyze conversations part {1}\",\n",
    "#                 \"analysisInput\": {\n",
    "#                     \"conversations\": [\n",
    "#                         {\n",
    "#                             \"conversationItems\": conversation_chunk,\n",
    "#                             \"modality\": \"text\",\n",
    "#                             \"id\": conversation_id,\n",
    "#                             \"language\": \"fr\",\n",
    "#                         },\n",
    "#                     ]\n",
    "#                 },\n",
    "#                 \"tasks\": [\n",
    "#                     {\n",
    "#                         \"taskName\": \"Title and chapter task\",\n",
    "#                         \"kind\": \"ConversationalSummarizationTask\",\n",
    "#                         \"parameters\": {\"summaryAspects\": [\"chapterTitle\"]},\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"taskName\": \"Recap task\",\n",
    "#                         \"kind\": \"ConversationalSummarizationTask\",\n",
    "#                         \"parameters\": {\"summaryAspects\": [\"recap\"]},\n",
    "#                     },\n",
    "#                 ],\n",
    "#             }\n",
    "#         )\n",
    "#         # Await the result of the poller\n",
    "#         result = await poller.result()\n",
    "#         view_result(result)\n",
    "\n",
    "#     finally:\n",
    "#         # Use `await` for closing the client\n",
    "#         await client.close()\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "# results = await analyze_conversations_async(LANGUAGE_ENDPOINT, LANGUAGE_KEY, formatted_paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poller = text_analytics_client.begin_analyze_actions(\n",
    "    documents,\n",
    "    display_name=\"Sample Text Analysis\",\n",
    "    actions=[\n",
    "        RecognizeEntitiesAction(),\n",
    "        RecognizePiiEntitiesAction(),\n",
    "        ExtractKeyPhrasesAction(),\n",
    "        RecognizeLinkedEntitiesAction(),\n",
    "        AnalyzeSentimentAction(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "document_results = poller.result()\n",
    "for doc, action_results in zip(documents, document_results):\n",
    "    print(f\"\\nDocument text: {doc}\")\n",
    "    for result in action_results:\n",
    "        if result.kind == \"EntityRecognition\":\n",
    "            print(\"...Results of Recognize Entities Action:\")\n",
    "            for entity in result.entities:\n",
    "                print(f\"......Entity: {entity.text}\")\n",
    "                print(f\".........Category: {entity.category}\")\n",
    "                print(f\".........Confidence Score: {entity.confidence_score}\")\n",
    "                print(f\".........Offset: {entity.offset}\")\n",
    "\n",
    "        elif result.kind == \"PiiEntityRecognition\":\n",
    "            print(\"...Results of Recognize PII Entities action:\")\n",
    "            for pii_entity in result.entities:\n",
    "                print(f\"......Entity: {pii_entity.text}\")\n",
    "                print(f\".........Category: {pii_entity.category}\")\n",
    "                print(f\".........Confidence Score: {pii_entity.confidence_score}\")\n",
    "\n",
    "        elif result.kind == \"KeyPhraseExtraction\":\n",
    "            print(\"...Results of Extract Key Phrases action:\")\n",
    "            print(f\"......Key Phrases: {result.key_phrases}\")\n",
    "\n",
    "        elif result.kind == \"EntityLinking\":\n",
    "            print(\"...Results of Recognize Linked Entities action:\")\n",
    "            for linked_entity in result.entities:\n",
    "                print(f\"......Entity name: {linked_entity.name}\")\n",
    "                print(f\".........Data source: {linked_entity.data_source}\")\n",
    "                print(f\".........Data source language: {linked_entity.language}\")\n",
    "                print(\n",
    "                    f\".........Data source entity ID: {linked_entity.data_source_entity_id}\"\n",
    "                )\n",
    "                print(f\".........Data source URL: {linked_entity.url}\")\n",
    "                print(\".........Document matches:\")\n",
    "                for match in linked_entity.matches:\n",
    "                    print(f\"............Match text: {match.text}\")\n",
    "                    print(f\"............Confidence Score: {match.confidence_score}\")\n",
    "                    print(f\"............Offset: {match.offset}\")\n",
    "                    print(f\"............Length: {match.length}\")\n",
    "\n",
    "        elif result.kind == \"SentimentAnalysis\":\n",
    "            print(\"...Results of Analyze Sentiment action:\")\n",
    "            print(f\"......Overall sentiment: {result.sentiment}\")\n",
    "            print(\n",
    "                f\"......Scores: positive={result.confidence_scores.positive}; \\\n",
    "                neutral={result.confidence_scores.neutral}; \\\n",
    "                negative={result.confidence_scores.negative} \\n\"\n",
    "            )\n",
    "\n",
    "        elif result.is_error is True:\n",
    "            print(\n",
    "                f\"...Is an error with code '{result.error.code}' and message '{result.error.message}'\"\n",
    "            )\n",
    "\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "# # Combine results as needed\n",
    "# for idx, result in enumerate(results):\n",
    "#     print(f\"Results for part {idx + 1}: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = \" \".join([results[i]['tasks']['items'][0]['results']['conversations'][0]['summaries'][0]['text'] for i in range(len(results))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive summarization: Produces a summary by extracting sentences that collectively represent the most important or relevant information within the original content.\n",
    "\n",
    "Abstractive summarization: Produces a summary by generating summarized sentences from the document that capture the main idea.\n",
    "\n",
    "Query-focused summarization: Allows you to use a query when summarizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(LANGUAGE_ENDPOINT, AzureKeyCredential(LANGUAGE_KEY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poller = text_analytics_client.begin_extract_summary([all_texts])\n",
    "extract_summary_results = poller.result()\n",
    "for result in extract_summary_results:\n",
    "    if result.kind == \"ExtractiveSummarization\":\n",
    "        print(\"Summary extracted: \\n{}\".format(\n",
    "            \" \".join([sentence.text for sentence in result.sentences]))\n",
    "        )\n",
    "    elif result.is_error is True:\n",
    "        print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "            result.error.code, result.error.message\n",
    "        ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CW_Azure-services",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
