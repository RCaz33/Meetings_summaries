{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cw-language-summary-meetings.cognitiveservices.azure.com/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "LANGUAGE_KEY = os.getenv('LANGUAGE_KEY')\n",
    "LANGUAGE_ENDPOINT = os.getenv('LANGUAGE_ENDPOINT')\n",
    "LANGUAGE_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx \n",
    "\n",
    "\n",
    "# path = \"data/2024_11_22_Point avancement Digitalisation.docx\"\n",
    "doc = docx.Document('data/Appel avec Remi Cazelles-brut.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    print(doc.paragraphs[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predefined Roles:\n",
    "\n",
    "\"Customer\":\n",
    "\"Agent\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "people = (\"list of names from the transcript file\")\n",
    "company = dict({\"name1\":{'id':1,\n",
    "                                    'role':'Customer'},\n",
    "                  \"name2\":{'id':2,\n",
    "                                   'role':'Customer'},\n",
    "                  \"name3\":{'id':3,\n",
    "                                   'role':'Assistant'},\n",
    "                  \"name4\":{'id':4,\n",
    "                                    'role':'Customer'},                    \n",
    "                  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.paragraphs[12].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def assign_role_and_id(line):\n",
    "#     nom = \" \".join(line.split(\" \")[:2])\n",
    "#     participant_id = company[nom][\"id\"]\n",
    "#     role = company[nom][\"role\"]\n",
    "#     return participant_id, role\n",
    "\n",
    "\n",
    "\n",
    "# formatted_paragraphs=list()\n",
    "\n",
    "# for i, paragraph in enumerate(doc.paragraphs):\n",
    "#     # Skip first empty paragraphs\n",
    "#     if i >1:\n",
    "    \n",
    "#         if not len(paragraph.text.split(\"\\n\")) > 2:\n",
    "#             continue\n",
    "        \n",
    "#         # print(paragraph.text)\n",
    "        \n",
    "#         text=list()\n",
    "#         # Format each paragraph as a dictionary\n",
    "#         for line in paragraph.text.split(\"\\n\"):\n",
    "            \n",
    "#             if line.startswith(people):  \n",
    "#                 participant_id, role = assign_role_and_id(line)\n",
    "                \n",
    "#             elif line.startswith(\"Remi Cazelles\"):    ### quel nom pour invit√© ou .. ?\n",
    "#                 participant_id = 2\n",
    "#                 role = \"Cyber team\"\n",
    "#             else:\n",
    "#                 if line:\n",
    "#                     text.append(line.replace(\"\\xa0\",\"\"))\n",
    "                \n",
    "#         formatted_paragraph = {\n",
    "#             \"text\": \". \".join(text),\n",
    "#             \"id\": str(i),  # Assign a unique ID based on the order\n",
    "#             \"role\": role,      # Assign a role (adjust based on your document content)\n",
    "#             \"participantId\": participant_id\n",
    "#         }\n",
    "        \n",
    "#         # Append formatted paragraph to list\n",
    "#         formatted_paragraphs.append(formatted_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-services/language-service/summarization/quickstart?tabs=conversation-summarization%2Clinux&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_paragraphs[11:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# format 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print()\n",
    "    print(doc.paragraphs[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64258 characters\n",
      "16064.5 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def assign_role_and_id(line):\n",
    "    nom = \" \".join(line.split(\" \")[:2])\n",
    "    participant_id = company[nom][\"id\"]\n",
    "    role = company[nom][\"role\"]\n",
    "    return participant_id, role\n",
    "\n",
    "formatted_paragraphs=list()\n",
    "characters_count=0\n",
    "for i, paragraph in enumerate(doc.paragraphs):\n",
    "    text=list()\n",
    "    if i <= 2:\n",
    "        continue\n",
    "    # Format each paragraph as a dictionary\n",
    "    for line in paragraph.text.split(\"\\n\"):\n",
    "        \n",
    "        if line == \"\":\n",
    "            continue\n",
    "        \n",
    "        elif line.startswith(people):  \n",
    "            participant_id, role = assign_role_and_id(line)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            if line:\n",
    "                text.append(line.replace(\"\\xa0\",\"\"))\n",
    "                \n",
    "                characters_count += len(line.replace(\"\\xa0\",\"\"))\n",
    "            \n",
    "    formatted_paragraph = {\n",
    "        \"text\": \". \".join(text),\n",
    "        \"id\": str(i),  # Assign a unique ID based on the order\n",
    "        \"role\": role,      # Assign a role (adjust based on your document content)\n",
    "        \"participantId\": participant_id\n",
    "    }\n",
    "    \n",
    "    # Append formatted paragraph to list\n",
    "    formatted_paragraphs.append(formatted_paragraph)\n",
    "    \n",
    "print(characters_count,\"characters\")\n",
    "print(characters_count/4,\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make chunks of conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def get_text_length(text):\n",
    "    \"\"\"Calculate the length of a string in Unicode text elements.\"\"\"\n",
    "    return sum(1 for _ in unicodedata.normalize('NFC', text))\n",
    "\n",
    "def chunk_text_by_length(conversation, max_length):\n",
    "    \"\"\"Split the conversation into chunks based on text length in Unicode elements.\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for item in conversation:\n",
    "        text_length = get_text_length(item['text'])\n",
    "        if current_length + text_length <= max_length:\n",
    "            current_chunk.append(item)\n",
    "            current_length += text_length\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [item]\n",
    "            current_length = text_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text_by_length(formatted_paragraphs,max_length=5200)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other preconfigured features (synchronous)\t5,120 as measured by StringInfo.LengthInTextElements. If you need to submit larger documents, consider using the feature asynchronously.\n",
    "All other preconfigured features (asynchronous)\t125,000 characters across all submitted documents, as measured by StringInfo.LengthInTextElements (maximum of 25 documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to read output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_result(result):\n",
    "    task_results = result[\"tasks\"][\"items\"]\n",
    "    for task in task_results:\n",
    "        print(f\"\\n{task['taskName']} status: {task['status']}\")\n",
    "        task_result = task[\"results\"]\n",
    "        if task_result[\"errors\"]:\n",
    "            print(\"... errors occurred ...\")\n",
    "            for error in task_result[\"errors\"]:\n",
    "                print(error)\n",
    "        else:\n",
    "            conversation_result = task_result[\"conversations\"][0]\n",
    "            if conversation_result[\"warnings\"]:\n",
    "                print(\"... view warnings ...\")\n",
    "                for warning in conversation_result[\"warnings\"]:\n",
    "                    print(warning)\n",
    "            else:\n",
    "                summaries = conversation_result[\"summaries\"]\n",
    "                for summary in summaries:\n",
    "                    print(f\"{summary['aspect']}: {summary['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appel API Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.core.credentials import AzureKeyCredential\n",
    "# from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "\n",
    "\n",
    "# conversation_id = 'start_meeting'#path.split(\"/\")[1].split(\".\")[0]\n",
    "\n",
    "# client = ConversationAnalysisClient(LANGUAGE_ENDPOINT, AzureKeyCredential(LANGUAGE_KEY))\n",
    "# with client:\n",
    "#     poller = client.begin_conversation_analysis(\n",
    "#         task={\n",
    "#             \"displayName\": \"Analyze conversations from xxx\",\n",
    "#             \"analysisInput\": {\n",
    "#                 \"conversations\": [\n",
    "#                     {\n",
    "#                         \"conversationItems\": formatted_paragraphs[:200],              # LIMIT HERE THE CONVERSATION LENGHT TO LIMIT COSTS\n",
    "#                         \"modality\": \"text\",\n",
    "#                         \"id\": conversation_id,\n",
    "#                         \"language\": \"fr\",\n",
    "#                     },\n",
    "#                 ]\n",
    "#             },\n",
    "#             \"tasks\": [\n",
    "#                 {\n",
    "#                     \"taskName\": \"Issue task\",\n",
    "#                     \"kind\": \"ConversationalSummarizationTask\",\n",
    "#                     \"parameters\": {\"summaryAspects\": [\"issue\"]},\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"taskName\": \"Resolution task\",\n",
    "#                     \"kind\": \"ConversationalSummarizationTask\",\n",
    "#                     \"parameters\": {\"summaryAspects\": [\"resolution\"]},\n",
    "#                 },\n",
    "#                     {\n",
    "#         \"taskName\": \"Recap task\",\n",
    "#         \"kind\": \"ConversationalSummarizationTask\",\n",
    "#         \"parameters\": {\"summaryAspects\": [\"recap\"]},\n",
    "#     },\n",
    "#             ],\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "    \n",
    "\n",
    "#     # view result\n",
    "#     result = poller.result()\n",
    "#     task_results = result[\"tasks\"][\"items\"]\n",
    "#     for task in task_results:\n",
    "#         print(f\"\\n{task['taskName']} status: {task['status']}\")\n",
    "#         task_result = task[\"results\"]\n",
    "#         if task_result[\"errors\"]:\n",
    "#             print(\"... errors occurred ...\")\n",
    "#             for error in task_result[\"errors\"]:\n",
    "#                 print(error)\n",
    "#         else:\n",
    "#             conversation_result = task_result[\"conversations\"][0]\n",
    "#             if conversation_result[\"warnings\"]:\n",
    "#                 print(\"... view warnings ...\")\n",
    "#                 for warning in conversation_result[\"warnings\"]:\n",
    "#                     print(warning)\n",
    "#             else:\n",
    "#                 summaries = conversation_result[\"summaries\"]\n",
    "#                 for summary in summaries:\n",
    "#                     print(f\"{summary['aspect']}: {summary['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# appel API Azure chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "\n",
    "\n",
    "# conversation_chunks = chunk_text_by_length(formatted_paragraphs,max_length=4500)\n",
    "\n",
    "conversation_chunks = chunks\n",
    "\n",
    "client = ConversationAnalysisClient(LANGUAGE_ENDPOINT, AzureKeyCredential(LANGUAGE_KEY))\n",
    "\n",
    "try:\n",
    "    results = []\n",
    "    for idx, chunk in enumerate(conversation_chunks):\n",
    "        conversation_id = f\"meetingWenboRemi_part_{idx + 1}\"\n",
    "        poller = client.begin_conversation_analysis(\n",
    "            task={\n",
    "                \"displayName\": f\"Analyze conversations part {idx + 1}\",\n",
    "                \"analysisInput\": {\n",
    "                    \"conversations\": [\n",
    "                        {\n",
    "                            \"conversationItems\": chunk,\n",
    "                            \"modality\": \"text\",\n",
    "                            \"id\": conversation_id,\n",
    "                            \"language\": \"fr\",\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "                \"tasks\": [\n",
    "                    # {\n",
    "                    #     \"taskName\": \"Issue task\",\n",
    "                    #     \"kind\": \"ConversationalSummarizationTask\",\n",
    "                    #     \"parameters\": {\"summaryAspects\": [\"issue\"]},\n",
    "                    # },\n",
    "                    # {\n",
    "                    #     \"taskName\": \"Resolution task\",\n",
    "                    #     \"kind\": \"ConversationalSummarizationTask\",\n",
    "                    #     \"parameters\": {\"summaryAspects\": [\"resolution\"]},\n",
    "                    # },\n",
    "                    {\n",
    "                        \"taskName\": \"Recap task\",\n",
    "                        \"kind\": \"ConversationalSummarizationTask\",\n",
    "                        \"parameters\": {\"summaryAspects\": [\"recap\"]},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        result = poller.result()\n",
    "        results.append(result)\n",
    "        view_result(result)\n",
    "        \n",
    "finally:\n",
    "    client.close()\n",
    "\n",
    "        \n",
    "\n",
    "# Combine results as needed\n",
    "for idx, result in enumerate(results):\n",
    "    print(f\"Results for part {idx + 1}: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view result\n",
    "result = poller.result()\n",
    "\n",
    "def view_result(result):\n",
    "    task_results = result[\"tasks\"][\"items\"]\n",
    "    for task in task_results:\n",
    "        print(f\"\\n{task['taskName']} status: {task['status']}\")\n",
    "        task_result = task[\"results\"]\n",
    "        if task_result[\"errors\"]:\n",
    "            print(\"... errors occurred ...\")\n",
    "            for error in task_result[\"errors\"]:\n",
    "                print(error)\n",
    "        else:\n",
    "            conversation_result = task_result[\"conversations\"][0]\n",
    "            if conversation_result[\"warnings\"]:\n",
    "                print(\"... view warnings ...\")\n",
    "                for warning in conversation_result[\"warnings\"]:\n",
    "                    print(warning)\n",
    "            else:\n",
    "                summaries = conversation_result[\"summaries\"]\n",
    "                for summary in summaries:\n",
    "                    print(f\"{summary['aspect']}: {summary['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate token usage\n",
    "\"\"\"\n",
    "While not a precise way of tracking tokens, you can estimate token usage based on the size of the input text you're sending. The number of tokens depends on the service, but you can approximate that:\n",
    "\n",
    "1 token is roughly 4 characters (or about 3/4 of a word).\n",
    "For example, if you are sending a string of 400 characters, you can estimate that it uses around 100 tokens.\n",
    "You can manually log the input size (character count) as a proxy for token usage, but the exact number of tokens can vary depending on the language model and other factors.\n",
    "\"\"\"\n",
    "\n",
    "## Track Token Usage via Azure Monitor\n",
    "\n",
    "If you're looking for more detailed token usage statistics, Azure's Monitor and Cost Management tools are useful. Here's how you can programmatically track token usage:\n",
    "\n",
    "Using Azure Monitor and Azure SDK for Python\n",
    "Set up monitoring in Azure Monitor for the Cognitive Service resource.\n",
    "Use Azure Monitor metrics to track the token usage for your Cognitive Services resource.\n",
    "You can query metrics using the azure-mgmt-monitor SDK to get detailed data about token consumption (e.g., TotalTokens metric).\n",
    "\n",
    "\n",
    "```from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.monitor import MonitorManagementClient\n",
    "from azure.mgmt.monitor.models import MetricAggregationType\n",
    "\n",
    "# Initialize the MonitorManagementClient\n",
    "credential = DefaultAzureCredential()\n",
    "monitor_client = MonitorManagementClient(credential, subscription_id)\n",
    "\n",
    "# Specify the resource ID of the Cognitive Services\n",
    "resource_id = \"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.CognitiveServices/accounts/{account_name}\"\n",
    "\n",
    "# Get metrics for the cognitive service\n",
    "metrics = monitor_client.metrics.list(\n",
    "    resource_id,\n",
    "    timespan=\"2024-11-01T00:00:00Z/2024-11-12T00:00:00Z\",  # Specify your timespan\n",
    "    metricnames=\"TotalTokens\",  # Metric to track token usage\n",
    "    aggregation=MetricAggregationType.SUM\n",
    ")\n",
    "\n",
    "for metric in metrics.value:\n",
    "    print(f\"Metric: {metric.name.localized_value}\")\n",
    "    for time_series in metric.timeseries:\n",
    "        for data in time_series.data:\n",
    "            print(f\"Timestamp: {data.timestamp}, Total Tokens: {data.total}\")```\n",
    "\n",
    "\n",
    "## Check Azure Cost Management Programmatically\n",
    "\n",
    "```from azure.mgmt.costmanagement import CostManagementClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Initialize the CostManagementClient\n",
    "credential = DefaultAzureCredential()\n",
    "cost_client = CostManagementClient(credential, subscription_id)\n",
    "\n",
    "# Get cost data for a particular period\n",
    "cost_data = cost_client.query.usage(\n",
    "    scope=\"/subscriptions/{subscription_id}\",\n",
    "    time_period={\"from\": \"2024-11-01\", \"to\": \"2024-11-12\"},\n",
    "    dataset={\"granularity\": \"Daily\"}\n",
    ")\n",
    "\n",
    "for entry in cost_data.value:\n",
    "    print(f\"Service: {entry['name']}, Cost: {entry['cost']}\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure.ai.language.conversations.aio.ConversationAnalysisClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(task_result,\"data/conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "unique_conversation_id = uuid.uuid1()\n",
    "str(unique_conversation_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CW_Azure-services",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
